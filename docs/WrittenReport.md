The first algorithm we implemented was the breadth first search (BFS) algorithm. This algorithm was essential for us to travel from node to node when we calculated shortest paths that were used in our other algorithm, betweenness centrality. We decided to use BFS for our traversal instead of BFS because for the way our data is structured, BFS would allow a more practical and efficient way to get the shortest path between nodes. We structured BFS similarly to how it was structured in mp_traversals and implemented a begin, end, add, pop, peek, and empty method. In our graph traversal iterator class, like any iterator, we implemented ++,* and != methods. This algorithm was very similar to what we used in mp_traversals but we added slight changes to make it more useful for finding the shortest path between nodes. 

The BFS class calls many of the underlying functions in the GraphTraversal::Iterator class. We keep track of the predecessor in this class and update it in the ++ operator. We chose to do this because it would help keep track of how we are travelling through the network for our shortest path methods. 

We implemented Kruskal’s algorithm in our functions.cc file. The purpose of Kruskal’s is to find the minimum spanning tree using the minimum weights of the edges. Weights are determined by getting the average number of connections of the two nodes the edge joins. We divide this number by one, so the minimal spanning tree goes through the edges with the most connections  The significance of this algorithm is to find the most popular spanning tree between people who have more connections.

Our Kruskal’s method takes in a string filename and an int numPeople, we used primitive data structures here so that it wouldn’t affect runtime too much while making copies. The input string is used to read data from our file and the numPeople is used to initialize our adjacency matrix and to initialize the number of people in our DisjointSet forests. We internally used Disjoint sets because it made the most sense for lookup and for unioning two vertices that share the current minimally weighted edge to keep track of the span of our minimum spanning tree for Kruskal’s algorithm. Both union and find for disjoint sets would be O(log n). We use a 2D vector of doubles as our queue as this seemed to be enough rather than using a priority queue such as a heap because the scope was only limited to usage in this algorithm’s method. At the end of this function we represent a 2D vector of doubles where each row signifies the edge and its first index is the weight of an edge which is a double since the weights may be double based on our implementation and the second two indices contain vertices or the two people whose share an edge. Note the people are represented by labels or numbers that align with the entries 0-4038.

For our Betweeness Centrality function we used BFS to find the shortest path between two given people and then we calculated the number of shortest paths that pass through a given person which would return us the betweenness of this person. This is significant because it allows us to visualize the significant connections of any given person. For instance, if someone had to look through each person's friends in the fastest way to get from one person to find another person, what is the fewest number of people we would have to look through, therefore our weights are 1 divided by the average of the degree of two vertices because the larger the degree the less bias we want to give to exploring that vertex.  Our betweenness centrality for all methods returns a vector of in, where each index represents the person and the value of the given index represents the betweenness centrality for the respective person. We input a filename and the numberofPeople in order again, to produce an accurate adjacency matrix which is all we really need to use to find betweenness centrality. Internally we utilize maps to keep track of every pair of vertices and the shortest path between them. The map’s key is the pair of vertices, and the value is a vector of ints represents the vertices of the shortest path between the pair. We chose to use a map because it provides for an efficient way to get a path given 2 nodes. We also rely on our implementation of BFS as this algorithm helps us find the shortest path from any given person to every other person on our graph.
 
The graph that is made from the file test_data.txt can be found in docs/Graph A.png

The graph that is made from the test_bigger_data.txt can be found in docs/Graph B.png

The graph that is made from the less_connected_data.txt can be found in docs/Graph C.png
The above graphs were used to test all of our algorithms. 

To test our adjacency matrix we used Graph A and Graph B. These graphs are small enough that we could manually figure out what should be in each spot in the graph so we created a 2d vector with the expected values and checked that the 2d vector returned by the adjacency matrix function returned the same thing.
To test BFS we used Graph A. We used the exact same test cases that were given in mp_traversals and tweaked them to fit Graph A. BFS is also used in the shortest path algorithms so it was also tested in the shortest path tests.

To test Kruskal’s Algorithm we used Graph A and Graph B. These graphs are small enough that we could manually figure out what the minimal spanning tree is so we created a 2d vector with the expected values and checked that the 2d vector returned by the kruskal's algorithm function returned the same thing.

To test the Shortest Path algorithm that is used in Betweenness Centrality we used Graphs A, B and C. We used the site https://graphonline.ru/en/create_graph_by_matrix to get the shortest path between different nodes in our graph and made vectors for each of these paths. We then tested that our shortest paths functions were returning the expected values by comparing them to these vectors.

To test Betweenness Centrality we use Graphs A and B. After making sure that our shortest path algorithm worked we manually counted the betweenness centrality for each node in each Graph and checked that the betweenness centrality was returning these expected values. 
 
Our leading question is the following: What is the most efficient way to disseminate information given a network of people and which people in the network are the most influential in this spreading of information? We want to use data from Facebook to look at the connections between people in order to answer this question. One of the algorithms we chose was Kruskal’s Algorithm. The reason we chose this is because it would return a minimal spanning tree, which would tell us how to reach every node with the minimum possible edge weight, which would be the fastest and most efficient method of spreading information. The weight of each edge was one divided by the average number of the two node’s connections. This means two nodes with a higher average number of connections would have a lower edge weight. The minimal spanning tree shows us a path to reach every node through users with the highest amount of connections, which shows how we can spread the most information efficiently. 

After running Kruskal’s algorithm on the dataset we found that the minimal spanning tree contains 4038 edges and starts with the edge between person 1 and person 0 and ends with the edge between person 4037 and 3980. We also calculated each node's betweenness centrality by calculating the shortest path between any two nodes in the network, using BFS, and seeing how many times each node appeared in the shortest path. Nodes with higher betweenness centralities are more influential and would be better people to pass information through. This is because they have the most shortest paths going through them, so they would have access to the most people and would thus heavily influence the spread of information. Because the betweenness centrality algorithm had an extremely long runtime, we were not able to run it on our target dataset from Facebook, but we still tested on smaller datasets and verified its accuracy. For this reason, we know our algorithm is functional, but we could not find the most influential node in the Facebook data, only on other smaller datasets. However, we were able to calculate the shortest path between 2 random nodes in the network with our shortest path algorithm that uses BFS, which did work on the target data set.

Something interesting that our results made us think of is how important efficiency really is. After trying to run our betweenness centrality we realized that at the rate it was going it would take way too long to calculate the betweenness centrality of each node in a network of 4039 people. There are obviously more than 4039 people that use Facebook and they are able to do analysis on their user data in a much faster time, which is really motivating to think about. It made us realize the potential we have with the skills we’re going to gain in the next 4 years. Besides learning how to implement useful algorithms we hadn’t known about before, this project taught us a lot about how to work in a team and create something from scratch. The process of delegating work fairly and making sure resources were being used efficiently helped us a lot in our planning and leadership skills. The task of starting with nothing and creating a fairly large project from scratch also really developed our programming skills. We learned how to format projects, write sufficient test cases for each part of the project and navigate git version control to make sure we could work in parallel. 

If given more time we would explore the average number of degrees of separation between each node. Before starting the project we learned that ​​within the US,  the average degree of separation between any two people is 3.46. We thought this was interesting and wanted to try verifying the 3.46 number using Facebook’s data, but unfortunately did not get the chance. We would definitely like to explore that in the future. Given more time, we would also like to try to find a more efficient way to calculate the betweenness centrality for each node in a large dataset. At the moment, our algorithm has a runtime of O(n^2) and is inefficient even though it works. At the moment, we are calculating the shortest path between every single node in the network and storing it in a map. We then go through each path for every single node and check to see if the node is present in the path, then add to the betweenness centrality. This means, on top of calculating every single shortest path, each path is being looked through 4039 times. To optimize this, we can instead go through each path and add to the betweenness centrality of each node if they are present in the path. This would mean each path is only being looked through once and would greatly minimize the runtime for betweenness centrality.